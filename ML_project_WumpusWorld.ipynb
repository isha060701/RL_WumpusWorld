{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_project_WumpusWorld.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONbzr5po7EHQ0vzyHPCCWy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isha060701/RL_WumpusWorld/blob/main/ML_project_WumpusWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -NotebookApp.iopub_data_rate_limit=1.0e10 \n",
        "import numpy as np\n",
        "\n",
        "# =============================================================================\n",
        "# definition of environment and inputs\n",
        "# =============================================================================\n",
        "#the shape of the environment\n",
        "environment_rows = 5 \n",
        "environment_columns = 5\n",
        "\n",
        "start = (0, 0) #start point\n",
        "\n",
        "#hole1 = (2, 0) #hole1 point\n",
        "#hole2 = (2, 1) #hole2 point\n",
        "#wall = (3, 2) #wall point\n",
        "\n",
        "goal = (4,4) #goal point\n",
        "\n",
        "episodes = 1200 #one sequence of states, actions, and rewards, which ends with a terminal state\n",
        "\n",
        "epsilon = 0.8 #the percentage of time when we should take the best action (instead of a random action)\n",
        "discount_factor = 0.5 #discount factor for future rewards\n",
        "\n",
        "# =============================================================================\n",
        "# Q-Learning class\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "#q_values = np.zeros((environment_rows, environment_columns, 4))\n",
        "dq_values = {}\n",
        "class Q_Learning:\n",
        "    \n",
        "    def __init__(self, environment_rows, environment_columns, start, hole1, hole2,hole3, hole4,  goal, episodes, epsilon):\n",
        "        self.environment_rows = environment_rows\n",
        "        self.environment_columns = environment_columns\n",
        "        self.start = start\n",
        "        self.goal=goal\n",
        "        self.hole1 = hole1\n",
        "        self.hole2 = hole2\n",
        "        self.hole3 = hole3\n",
        "        self.hole4 = hole4\n",
        "        # self.hole5 = hole5\n",
        "        # self.hole6 = hole6\n",
        "        self.goal = goal\n",
        "        self.episodes = episodes\n",
        "        self.epsilon = epsilon\n",
        "        self.discount_factor = discount_factor\n",
        "        self.flag=0\n",
        "        self.flag_test_train=0\n",
        "        \n",
        "    def set_matrices(self):\n",
        "        #create a 2D numpy array to hold the rewards for each state\n",
        "        self.rewards = np.full((self.environment_rows, self.environment_columns), -1)\n",
        "        self.visited = np.full((self.environment_rows, self.environment_columns), 0)\n",
        "        self.rewards[self.hole1] = self.rewards[self.hole2]= self.rewards[self.hole3]= self.rewards[self.hole4]   = -100\n",
        "        self.rewards[self.goal] = 100\n",
        "        self.visited[self.hole1] = self.visited[self.hole2]= self.visited[self.hole3]= self.visited[self.hole4]   = 1\n",
        "        #create a 3D numpy array to hold the current Q-values for each state and action pair: Q(s, a)\n",
        "        # for i in range(environment_rows):\n",
        "        #   for j in range(environment_columns):\n",
        "        #     coordinate=(i,j)\n",
        "        #     dq_values.update({coordinate:{}})\n",
        "\n",
        "        #define actions\n",
        "        #numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
        "        self.actions = ['up', 'right', 'down', 'left']\n",
        "    \n",
        "    #define a function that determines if the specified location is a terminal state\n",
        "    def is_terminal_state(self, current_row_index, current_column_index):\n",
        "        if self.rewards[current_row_index, current_column_index] == -1:\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "    def get_quad(self,current_row_index,current_column_index):\n",
        "      quad=0\n",
        "      if current_row_index<=(int(environment_rows/2)-1) and current_column_index<=(int(environment_columns/2)-1):\n",
        "        quad=1\n",
        "      elif current_row_index>(int(environment_rows/2)-1) and current_column_index<=(int(environment_columns/2)-1):\n",
        "        quad=3\n",
        "      elif current_column_index>(int(environment_columns/2)-1) and current_row_index<=(int(environment_rows/2)-1):\n",
        "        quad=2\n",
        "      elif current_column_index>(int(environment_columns/2)-1) and current_row_index>(int(environment_rows/2)-1):\n",
        "        quad=4\n",
        "      return quad\n",
        "\n",
        "    def append_state(self,current_row_index, current_column_index):\n",
        "      if current_row_index-1<0:\n",
        "        rewup=-99999\n",
        "      else:\n",
        "        rewup= self.rewards[current_row_index-1,current_column_index]\n",
        "      if current_column_index+1>=self.environment_columns:\n",
        "        rewright=-99999\n",
        "      else:\n",
        "        rewright=self.rewards[current_row_index,current_column_index+1]\n",
        "      if current_row_index+1>=self.environment_rows:\n",
        "        rewdown=-99999\n",
        "      else:\n",
        "        rewdown=self.rewards[current_row_index+1,current_column_index]\n",
        "      if current_column_index-1<0:\n",
        "        rewleft=-99999\n",
        "      else:\n",
        "        rewleft= self.rewards[current_row_index,current_column_index-1]\n",
        "      \n",
        "      quad=self.get_quad(current_row_index,current_column_index)\n",
        "      \n",
        "      state=(quad,rewup,rewright,rewdown,rewleft)\n",
        "      #coord=(current_row_index,current_column_index)\n",
        "      if state not in dq_values:     #if state not present add in table\n",
        "            if(self.flag_test_train==0):\n",
        "              dq_values.update({state:[0,0,0,0]})\n",
        "            elif(self.flag_test_train==1):\n",
        "              for temp_quad in range(1,5):\n",
        "                if(temp_quad!=state[0]):\n",
        "                  temp_state=(temp_quad,state[1],state[2],state[3],state[4])\n",
        "                  if temp_state in dq_values:\n",
        "                    return temp_state\n",
        "              print(\"not found append_state\")\n",
        "      return state\n",
        "\n",
        "    #define an epsilon greedy algorithm that will choose which action to take next\n",
        "    def get_next_action(self, current_row_index, current_column_index, epsilon, episode):\n",
        "        #if a randomly chosen value between 0 and 1 is less than epsilon, \n",
        "        #then choose the most promising value from the Q-table for this state.\n",
        "        \n",
        "        state=self.append_state(current_row_index,current_column_index)\n",
        "        #print(episode)\n",
        "        #print(dq_values[state])\n",
        "\n",
        "        # if (episode<400):\n",
        "          \n",
        "        if self.flag_test_train==0:  #training\n",
        "          if np.random.random() < epsilon:   #exploit\n",
        "              return np.argmax(dq_values[state])\n",
        "              \n",
        "          else:                              #explore\n",
        "              return np.random.randint(4)\n",
        "        elif self.flag_test_train==1:   #testing\n",
        "          if state in dq_values:\n",
        "            return np.argmax(dq_values[state])\n",
        "          else :\n",
        "            for temp_quad in range(1,5):\n",
        "              if(temp_quad!=state[0]):\n",
        "                temp_state=(temp_quad,state[1],state[2],state[3],state[4])\n",
        "                if temp_state in dq_values:\n",
        "                  return np.argmax(dq_values[temp_state])\n",
        "            print(\"not found get next action\")\n",
        "              \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "           \n",
        "\n",
        "    #define a function that will get the next location based on the chosen action\n",
        "    def get_next_location(self, current_row_index, current_column_index, action_index):\n",
        "        new_row_index = current_row_index\n",
        "        new_column_index = current_column_index\n",
        "        if self.actions[action_index] == 'up' and current_row_index > 0:\n",
        "          #print(\"UP\")\n",
        "          new_row_index -= 1\n",
        "        elif self.actions[action_index] == 'right' and current_column_index < self.environment_columns - 1:\n",
        "          #print(\"RIGHT\")\n",
        "          new_column_index += 1\n",
        "        elif self.actions[action_index] == 'down' and current_row_index < self.environment_rows - 1:\n",
        "          #print(\"DOWN\")\n",
        "          new_row_index += 1\n",
        "        elif self.actions[action_index] == 'left' and current_column_index > 0:\n",
        "          #print(\"LEFT\")\n",
        "          new_column_index -= 1\n",
        "        # else :\n",
        "        #   print(\"terminal\")\n",
        "        return new_row_index, new_column_index\n",
        "\n",
        "\n",
        "    #run through 500 training episodes\n",
        "    def train(self):\n",
        "        for episode in range(self.episodes):\n",
        "            #get the starting location for this episode\n",
        "            curr_row_index, curr_column_index = self.start\n",
        "\n",
        "            #continue taking actions (i.e., moving) until we reach a terminal state\n",
        "            #(i.e., until we reach the item packaging area or crash into an item storage location)\n",
        "            while not self.is_terminal_state(curr_row_index, curr_column_index):\n",
        "      \n",
        "                #choose which action to take (i.e., where to move next)\n",
        "                action_index = self.get_next_action(curr_row_index, curr_column_index, self.epsilon,episode)\n",
        "\n",
        "                #store the old row and column indexes\n",
        "                old_row_index, old_column_index = curr_row_index, curr_column_index \n",
        "    \n",
        "                #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
        "                curr_row_index, curr_column_index = self.get_next_location(curr_row_index, curr_column_index, action_index)\n",
        "    \n",
        "                #receive the reward for moving to the new state, and calculate the temporal difference\n",
        "                \n",
        "\n",
        "                state_prev=self.append_state(old_row_index, old_column_index)\n",
        "                state=self.append_state(curr_row_index, curr_column_index)\n",
        "                reward = state_prev[action_index+1]\n",
        "                \n",
        "                if reward==100 and self.flag==0:\n",
        "                  print(\"goal found at\",(curr_row_index, curr_column_index))\n",
        "                  if curr_row_index>0:\n",
        "                    if self.rewards[curr_row_index-1, curr_column_index]==-1:\n",
        "                      self.rewards[curr_row_index-1, curr_column_index]=90\n",
        "                  if curr_column_index>0:\n",
        "                    if self.rewards[curr_row_index, curr_column_index-1]==-1:\n",
        "                      self.rewards[curr_row_index, curr_column_index-1]=90\n",
        "                  if curr_row_index< self.environment_rows - 1:\n",
        "                    if self.rewards[curr_row_index+1, curr_column_index]==-1:\n",
        "                      self.rewards[curr_row_index+1, curr_column_index]=90\n",
        "                  if curr_column_index< self.environment_columns - 1:\n",
        "                    if self.rewards[curr_row_index, curr_column_index+1]==-1:\n",
        "                      self.rewards[curr_row_index, curr_column_index+1]=90\n",
        "                  self.flag=1\n",
        "                  # print(\"changed\")\n",
        "                  # print(self.rewards)\n",
        "                \n",
        "                \n",
        "                new_q_value= reward + (self.discount_factor * np.max(dq_values[state]))  #Q-learning\n",
        "\n",
        "                #new_q_value= reward + (self.discount_factor * dq_values[state][action_index])    #SARSA\n",
        "                \n",
        "                # print(new_q_value)\n",
        "                #coord1=(old_row_index,old_column_index)\n",
        "                #print(dq_values[coord1][state_prev][action_index])\n",
        "                dq_values[state_prev][action_index]=new_q_value\n",
        "                # print(dq_values)\n",
        "        # for i in dq_values:\n",
        "        #   print (i, dq_values[i])\n",
        "        # print(len(dq_values))\n",
        "        print(\"----------------------\")   \n",
        "\n",
        "#testing\n",
        "    def get_shortest_path(self, start):\n",
        "        self.flag_test_train=1\n",
        "        start_row_index, start_column_index = start\n",
        "        #return immediately if this is an invalid starting location\n",
        "        if self.is_terminal_state(start_row_index, start_column_index):\n",
        "            return []\n",
        "        else: #if this is a 'legal' starting location\n",
        "            current_row_index, current_column_index = start_row_index, start_column_index\n",
        "            shortest_path = []\n",
        "            shortest_path.append([current_row_index, current_column_index])\n",
        "            print(current_row_index,current_column_index)\n",
        "\n",
        "            self.visited[current_row_index, current_column_index]=1;\n",
        "            while not self.is_terminal_state(current_row_index, current_column_index):\n",
        "                action_index = self.get_next_action(current_row_index, current_column_index, 1,300)\n",
        "                \n",
        "                temp_row,temp_col=self.get_next_location(current_row_index, current_column_index, action_index)\n",
        "\n",
        "                state=self.append_state(current_row_index,current_column_index)\n",
        "                temp_q_val= dq_values[state]\n",
        "\n",
        "                while(self.visited[temp_row,temp_col]==1):   #visited\n",
        "                  \n",
        "                  temp_q_val[action_index]=-99999\n",
        "                  next_highest_index=np.argmax(temp_q_val)\n",
        "                  if(temp_q_val[next_highest_index]==-99999): #all block\n",
        "                    return shortest_path\n",
        "                  temp_row,temp_col=self.get_next_location(current_row_index, current_column_index, next_highest_index)\n",
        "                current_row_index, current_column_index = temp_row,temp_col\n",
        "                \n",
        "                print(current_row_index,current_column_index)\n",
        "\n",
        "                shortest_path.append([current_row_index, current_column_index])\n",
        "                self.visited[current_row_index, current_column_index]=1;\n",
        "        return shortest_path\n",
        "\n",
        "\n",
        "    def get_board_path(self,start,shortest_path):\n",
        "      board=self.rewards\n",
        "      for moves in shortest_path:\n",
        "        if board[moves[0]][moves[1]]!=100:\n",
        "          board[moves[0]][moves[1]]=2\n",
        "        \n",
        "      for i in range(environment_rows):\n",
        "        for j in range(environment_columns):\n",
        "          if board[i][j]==-1:\n",
        "            print(\"_\",end=' ')\n",
        "          if board[i][j]==-100:\n",
        "            print(\"X\",end=' ')\n",
        "          if board[i][j]==100:\n",
        "            print(\"G\",end=' ')\n",
        "          if board[i][j]==2:\n",
        "            print(\"O\",end=' ')\n",
        "          if board[i][j]==90:\n",
        "            print(\"_\",end=' ')\n",
        "        print(\"\\n\")\n",
        "    \n",
        "\n",
        "    def get_board(self,start):\n",
        "      board=self.rewards\n",
        "      for i in range(environment_rows):\n",
        "        for j in range(environment_columns):\n",
        "          if board[i][j]==-1:\n",
        "            print(\"_\",end=' ')\n",
        "          if board[i][j]==-100:\n",
        "            print(\"X\",end=' ')\n",
        "          if board[i][j]==100:\n",
        "            print(\"G\",end=' ')\n",
        "          if board[i][j]==90:\n",
        "            print(\"_\",end=' ')\n",
        "        print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "3DACIeQ2hZWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # #random but similar boards\n",
        "# Q_L = Q_Learning(environment_rows, environment_columns, start, (1,3), (1,1), (1,2),(2,1),(2,2), episodes, epsilon)\n",
        "# Q_L.set_matrices()\n",
        "# Q_L.rewards\n",
        "# Q_L.train()\n",
        "# print(\"finished training for 1st case\")\n",
        "# Q_L.get_board(start)\n",
        "# #shortest_path=Q_L.get_shortest_path(start)\n",
        "# #print(shortest_path)\n",
        "# print(\"################################################\")\n",
        "# Q_L1 = Q_Learning(environment_rows, environment_columns, start, (1,2), (1,3), (2,1),(3,1),(2,2), episodes, epsilon)\n",
        "# Q_L1.set_matrices()\n",
        "# Q_L1.rewards\n",
        "# Q_L1.train()\n",
        "# print(\"finished training for 2nd case\")\n",
        "# Q_L1.get_board(start)\n",
        "# #shortest_path1=Q_L1.get_shortest_path(start)\n",
        "# #print(shortest_path1)\n",
        "# print(\"################################################\")\n",
        "# Q_L2 = Q_Learning(environment_rows, environment_columns, start, (0,2), (3,1), (1,2),(2,1),(2,2), episodes, epsilon)\n",
        "# Q_L2.set_matrices()\n",
        "# Q_L2.rewards\n",
        "# Q_L2.train()\n",
        "# print(\"finished training for 3rd case\")\n",
        "# Q_L2.get_board(start)\n",
        "# #shortest_path2=Q_L2.get_shortest_path(start)\n",
        "# #print(shortest_path1)\n",
        "# print(\"################################################\")\n",
        "# Q_L3 = Q_Learning(environment_rows, environment_columns, start, (1,2), (3,0), (0,2),(3,1),(2,2), episodes, epsilon)\n",
        "# Q_L3.set_matrices()\n",
        "# Q_L3.rewards\n",
        "# Q_L3.train()\n",
        "# print(\"finished training for 4th case\")\n",
        "# Q_L3.get_board(start)\n",
        "# #shortest_path3=Q_L3.get_shortest_path(start)\n",
        "# #print(shortest_path1)\n",
        "# print(\"################################################\")\n",
        "# Q_L4 = Q_Learning(environment_rows, environment_columns, start, (1,1), (1,2), (2,1),(4,0),(2,2), episodes, epsilon)\n",
        "# Q_L4.set_matrices()\n",
        "# Q_L4.rewards\n",
        "# Q_L4.train()\n",
        "# print(\"finished training for 5th case\")\n",
        "# Q_L4.get_board(start)\n",
        "# #shortest_path4=Q_L4.get_shortest_path(start)\n",
        "# #print(shortest_path1)\n",
        "# print(\"################################################\")\n",
        "# Q_L6 = Q_Learning(environment_rows, environment_columns, start, (1,1), (1,2), (2,1),(3,1),(2,2), episodes, epsilon)\n",
        "# Q_L6.set_matrices()\n",
        "# Q_L6.rewards\n",
        "# Q_L6.train()\n",
        "# print(\"finished training for 6th case\")\n",
        "# Q_L6.get_board(start)\n",
        "# #shortest_path6=Q_L6.get_shortest_path(start)\n",
        "# #print(shortest_path1)\n",
        "# print(\"################################################\")\n",
        "# Q_L5 = Q_Learning(environment_rows, environment_columns, start, (1,1), (1,2), (0,4),(2,1),(2,2), episodes, epsilon)\n",
        "# Q_L5.set_matrices()\n",
        "# Q_L5.rewards\n",
        "# #Q_L2.train()\n",
        "# print(\"Testing for 7th case\")\n",
        "# shortest_path5=Q_L5.get_shortest_path(start)\n",
        "# print(shortest_path5)\n",
        "# Q_L5.get_board_path(start,shortest_path5)\n",
        "\n",
        "\n",
        "\n",
        "Q_L = Q_Learning(environment_rows, environment_columns, start, (0,2), (0,3), (1,4),(2,0),(4,4), episodes, epsilon)\n",
        "Q_L.set_matrices()\n",
        "Q_L.rewards\n",
        "Q_L.train()\n",
        "print(\"finished training for 1st case\")\n",
        "Q_L.get_board(start)\n",
        "#shortest_path=Q_L.get_shortest_path(start)\n",
        "#print(shortest_path)\n",
        "print(\"################################################\")\n",
        "\n",
        "Q_L1 = Q_Learning(environment_rows, environment_columns, start, (0,4), (2,0), (2,1),(3,2),(4,4), episodes, epsilon)\n",
        "Q_L1.set_matrices()\n",
        "Q_L1.rewards\n",
        "Q_L1.train()\n",
        "print(\"finished training for 2nd case\")\n",
        "Q_L1.get_board(start)\n",
        "#shortest_path1=Q_L1.get_shortest_path(start)\n",
        "#print(shortest_path1)\n",
        "print(\"################################################\")\n",
        "\n",
        "\n",
        "Q_L2 = Q_Learning(environment_rows, environment_columns, start, (1,2), (1,4), (2,2),(4,0),(4,4), episodes, epsilon)\n",
        "Q_L2.set_matrices()\n",
        "Q_L2.rewards\n",
        "Q_L2.train()\n",
        "print(\"finished training for 3rd case\")\n",
        "Q_L2.get_board(start)\n",
        "#shortest_path2=Q_L2.get_shortest_path(start)\n",
        "#print(shortest_path1)\n",
        "print(\"################################################\")\n",
        "Q_L3 = Q_Learning(environment_rows, environment_columns, start, (0,2), (2,1), (2,2),(3,1),(4,4), episodes, epsilon)\n",
        "Q_L3.set_matrices()\n",
        "Q_L3.rewards\n",
        "Q_L3.train()\n",
        "print(\"finished training for 4th case\")\n",
        "Q_L3.get_board(start)\n",
        "#shortest_path3=Q_L3.get_shortest_path(start)\n",
        "#print(shortest_path1)\n",
        "print(\"################################################\")\n",
        "Q_L4 = Q_Learning(environment_rows, environment_columns, start, (1,0), (1,1), (1,3),(3,2),(4,4), episodes, epsilon)\n",
        "Q_L4.set_matrices()\n",
        "Q_L4.rewards\n",
        "Q_L4.train()\n",
        "print(\"finished training for 5th case\")\n",
        "Q_L4.get_board(start)\n",
        "#shortest_path4=Q_L4.get_shortest_path(start)\n",
        "#print(shortest_path1)\n",
        "print(\"################################################\")\n",
        "Q_L6 = Q_Learning(environment_rows, environment_columns, start, (0,2), (2,2), (4,0),(3,2),(4,4), episodes, epsilon)\n",
        "Q_L6.set_matrices()\n",
        "Q_L6.rewards\n",
        "Q_L6.train()\n",
        "print(\"finished training for 6th case\")\n",
        "Q_L6.get_board(start)\n",
        "#shortest_path6=Q_L6.get_shortest_path(start)\n",
        "#print(shortest_path1)\n",
        "print(\"################################################\")\n",
        "\n",
        "Q_L5 = Q_Learning(environment_rows, environment_columns, start, (0,2), (2,1), (3,2),(4,3),(4,4),\n",
        " episodes, epsilon)\n",
        "Q_L5.set_matrices()\n",
        "Q_L5.rewards\n",
        "#Q_L2.train()\n",
        "print(\"Testing for 7th case\")\n",
        "Q_L5.get_board(start)\n",
        "shortest_path5=Q_L5.get_shortest_path(start)\n",
        "\n",
        "print(shortest_path5)\n",
        "Q_L5.get_board_path(start,shortest_path5)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRrhpRFczyRB",
        "outputId": "665091ba-a6e4-4999-bab5-d2c209cfd01e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "goal found at (4, 4)\n",
            "----------------------\n",
            "finished training for 1st case\n",
            "_ _ X X _ \n",
            "\n",
            "_ _ _ _ X \n",
            "\n",
            "X _ _ _ _ \n",
            "\n",
            "_ _ _ _ _ \n",
            "\n",
            "_ _ _ _ G \n",
            "\n",
            "################################################\n",
            "goal found at (4, 4)\n",
            "----------------------\n",
            "finished training for 2nd case\n",
            "_ _ _ _ X \n",
            "\n",
            "_ _ _ _ _ \n",
            "\n",
            "X X _ _ _ \n",
            "\n",
            "_ _ X _ _ \n",
            "\n",
            "_ _ _ _ G \n",
            "\n",
            "################################################\n",
            "goal found at (4, 4)\n",
            "----------------------\n",
            "finished training for 3rd case\n",
            "_ _ _ _ _ \n",
            "\n",
            "_ _ X _ X \n",
            "\n",
            "_ _ X _ _ \n",
            "\n",
            "_ _ _ _ _ \n",
            "\n",
            "X _ _ _ G \n",
            "\n",
            "################################################\n",
            "goal found at (4, 4)\n",
            "----------------------\n",
            "finished training for 4th case\n",
            "_ _ X _ _ \n",
            "\n",
            "_ _ _ _ _ \n",
            "\n",
            "_ X X _ _ \n",
            "\n",
            "_ X _ _ _ \n",
            "\n",
            "_ _ _ _ G \n",
            "\n",
            "################################################\n",
            "goal found at (4, 4)\n",
            "----------------------\n",
            "finished training for 5th case\n",
            "_ _ _ _ _ \n",
            "\n",
            "X X _ X _ \n",
            "\n",
            "_ _ _ _ _ \n",
            "\n",
            "_ _ X _ _ \n",
            "\n",
            "_ _ _ _ G \n",
            "\n",
            "################################################\n",
            "goal found at (4, 4)\n",
            "----------------------\n",
            "finished training for 6th case\n",
            "_ _ X _ _ \n",
            "\n",
            "_ _ _ _ _ \n",
            "\n",
            "_ _ X _ _ \n",
            "\n",
            "_ _ X _ _ \n",
            "\n",
            "X _ _ _ G \n",
            "\n",
            "################################################\n",
            "Testing for 7th case\n",
            "_ _ X _ _ \n",
            "\n",
            "_ _ _ _ _ \n",
            "\n",
            "_ X _ _ _ \n",
            "\n",
            "_ _ X _ _ \n",
            "\n",
            "_ _ _ X G \n",
            "\n",
            "0 0\n",
            "0 1\n",
            "1 1\n",
            "1 2\n",
            "2 2\n",
            "2 3\n",
            "3 3\n",
            "3 4\n",
            "4 4\n",
            "[[0, 0], [0, 1], [1, 1], [1, 2], [2, 2], [2, 3], [3, 3], [3, 4], [4, 4]]\n",
            "O O X _ _ \n",
            "\n",
            "_ O O _ _ \n",
            "\n",
            "_ X O O _ \n",
            "\n",
            "_ _ X O O \n",
            "\n",
            "_ _ _ X G \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eeh4EYxhiOCW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}